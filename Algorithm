Also gone throught he algorithm for RL:

Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards and uses this to learn a policy that maximizes cumulative rewards over time. Four important RL algorithms often discussed are REINFORCE, Monte Carlo Policy Evaluation, Q-Learning, and SARSA. Each has distinct characteristics, advantages, and use cases.

REINFORCE is a foundational policy gradient algorithm in reinforcement learning. Unlike methods that estimate the value of actions or states, REINFORCE directly optimizes the agent's policy. It does so by collecting complete episodes of experience and then adjusting the policy parameters based on the total rewards received. The key idea is to increase the likelihood of actions that led to high rewards and reduce it for those with poor outcomes. REINFORCE is conceptually simple and well-suited for problems with continuous action spaces or where direct policy optimization is more effective than value-based approaches. However, it suffers from high variance and can be sample inefficient, as it requires full episodes for updates. Variants like REINFORCE with baseline or Actor-Critic methods have been proposed to address these issues.

Monte Carlo Policy Evaluation is used to estimate the value of a given policy by averaging the returns observed in many episodes. It does not involve any model of the environment and works well when episodes are finite. The method stores the total rewards following a state under the current policy and averages them to compute an estimate of the value function. This helps in understanding how good a certain policy is, without modifying it. It’s a crucial part of Monte Carlo Control, which combines policy evaluation and improvement. However, it only works when episodes terminate and can be slow to converge due to the variance in return estimates.

Q-Learning is a model-free, off-policy value-based algorithm. It learns an action-value function that tells the agent how good it is to take a certain action in a given state, regardless of the agent’s policy. The core idea is to update the Q-values using the Bellman equation, based on observed rewards and the maximum expected future rewards. Because it is off-policy, Q-Learning learns the optimal policy independently of the agent’s actions. This makes it stable and powerful for many practical tasks. It is widely used in environments where the agent needs to learn optimal strategies from scratch.

SARSA, which stands for State-Action-Reward-State-Action, is similar to Q-Learning but is an on-policy algorithm. It updates the action-value function using the action actually taken by the current policy. This means that SARSA learns about the policy it is currently following. As a result, it is more conservative than Q-Learning in some scenarios, especially when exploration strategies like epsilon-greedy are used. SARSA tends to perform better in environments where safety or consistency of behavior is crucial.
